{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"../../corpus/novel_formated/\"\n",
    "# corpus_name = \"novel_segments2.tsv\"\n",
    "corpus_name = \"ntt_segment.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "conv_data = []\n",
    "with open(out_path+corpus_name, \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    conv_data = [row for row in reader]\n",
    "\n",
    "conv_only_utt = [ [\"\".join(conv[-3:-1]), conv[-1]] for conv in conv_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "def make_triple(conv_data):\n",
    "    triple_list = []\n",
    "    last_utts = [ conv[-1] for conv in conv_data if len(conv[-1]) > 6 ]\n",
    "    neg_candidate = random.sample(last_utts, len(last_utts))\n",
    "    for conv, neg in zip(conv_data, neg_candidate):\n",
    "        # 短すぎる応答は殺す\n",
    "        if len(conv[1]) <= 6:\n",
    "            continue\n",
    "        triple_list.append( [*conv , neg] )\n",
    "\n",
    "    return triple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_list = make_triple(conv_only_utt)\n",
    "\n",
    "mini_size = 10000\n",
    "triple_list_mini = random.sample(triple_list,  mini_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# from sentence_transformers import models\n",
    "\n",
    "# bert_path = \"../../corpus/pretrained/sbert_unclear1\"\n",
    "bert_path = \"../../corpus/pretrained/sbert_coherence\"\n",
    "sbert = SentenceTransformer(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データ生成\n",
    "# 全データのエンコード\n",
    "triple_flatten = sum(triple_list_mini, [])\n",
    "triple_vec = sbert.encode(triple_flatten).reshape(len(triple_list_mini), 3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def vec2feature(x, y):\n",
    "    diff = np.abs( x - y )\n",
    "    return np.concatenate([x, y, diff])\n",
    "\n",
    "\n",
    "def make_X_y(triple_vec):\n",
    "    X = []\n",
    "    y = []\n",
    "    for anc, pos, neg in triple_vec:\n",
    "        # pos\n",
    "        pos_f = vec2feature(anc, pos)\n",
    "        X.append(pos_f)\n",
    "        y.append(1)\n",
    "        # neg\n",
    "        neg_f = vec2feature(anc, neg)\n",
    "        X.append(neg_f)\n",
    "        y.append(0)\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_X_y(triple_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=5, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "\n",
    "class CoherenceModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, tagset_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(CoherenceModel, self).__init__()    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hid1= embedding_dim*2\n",
    "        self.hid2 = embedding_dim//2\n",
    "        self.fc1 = nn.Linear(self.embedding_dim, self.hid1)\n",
    "        self.fc2 = nn.Linear(self.hid1, self.hid2)\n",
    "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
    "        # self.hidden2tag = nn.Linear(self.hid2+self.fb_dim, tagset_size)\n",
    "        self.hidden2tag = nn.Linear(self.hid2, tagset_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.fc1(x))\n",
    "        y = F.relu(self.fc2(y))\n",
    "        y = self.hidden2tag( y )\n",
    "        y = F.log_softmax(y, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "epoch_ = 100\n",
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304 4608 2\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 768*3\n",
    "HIDDEN_DIM = EMBEDDING_DIM*2\n",
    "OUTPUT_DIM = 2\n",
    "# seq_len = length\n",
    "print(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CoherenceModel(EMBEDDING_DIM, OUTPUT_DIM)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 \t loss 127.25350272655487\n",
      "epoch 10 \t loss 119.89729243516922\n",
      "epoch 15 \t loss 113.73343002796173\n",
      "epoch 20 \t loss 110.17737072706223\n",
      "epoch 25 \t loss 105.67075926065445\n",
      "epoch 30 \t loss 104.19298395514488\n",
      "epoch 35 \t loss 98.73773795366287\n",
      "epoch 40 \t loss 97.23666512966156\n",
      "epoch 45 \t loss 88.93959091603756\n",
      "epoch 50 \t loss 86.68876521289349\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/yamada/Documents/twitchAI/train/baseline1/train_coherence_dnn.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/yamada/Documents/twitchAI/train/baseline1/train_coherence_dnn.ipynb#ch0000019vscode-remote?line=12'>13</a>\u001b[0m score \u001b[39m=\u001b[39m model(X_t_tensor)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/yamada/Documents/twitchAI/train/baseline1/train_coherence_dnn.ipynb#ch0000019vscode-remote?line=13'>14</a>\u001b[0m loss_ \u001b[39m=\u001b[39m loss_function(score, y_t_tensor)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/yamada/Documents/twitchAI/train/baseline1/train_coherence_dnn.ipynb#ch0000019vscode-remote?line=14'>15</a>\u001b[0m loss_\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/yamada/Documents/twitchAI/train/baseline1/train_coherence_dnn.ipynb#ch0000019vscode-remote?line=15'>16</a>\u001b[0m all_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/yamada/Documents/twitchAI/train/baseline1/train_coherence_dnn.ipynb#ch0000019vscode-remote?line=16'>17</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/torch/tensor.py:245\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/yamada/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/torch/tensor.py?line=235'>236</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/yamada/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/torch/tensor.py?line=236'>237</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///home/yamada/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/torch/tensor.py?line=237'>238</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///home/yamada/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/torch/tensor.py?line=238'>239</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/yamada/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/torch/tensor.py?line=242'>243</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///home/yamada/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/torch/tensor.py?line=243'>244</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///home/yamada/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/torch/tensor.py?line=244'>245</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/torch/autograd/__init__.py:141\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/yamada/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/torch/autograd/__init__.py?line=137'>138</a>\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(inputs) \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[1;32m    <a href='file:///home/yamada/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/torch/autograd/__init__.py?line=139'>140</a>\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[0;32m--> <a href='file:///home/yamada/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/torch/autograd/__init__.py?line=140'>141</a>\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_)\n\u001b[1;32m    <a href='file:///home/yamada/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/torch/autograd/__init__.py?line=141'>142</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/yamada/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/torch/autograd/__init__.py?line=142'>143</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/torch/autograd/__init__.py:51\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     <a href='file:///home/yamada/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/torch/autograd/__init__.py?line=48'>49</a>\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     <a href='file:///home/yamada/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/torch/autograd/__init__.py?line=49'>50</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='file:///home/yamada/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/torch/autograd/__init__.py?line=50'>51</a>\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39;49mones_like(out, memory_format\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mpreserve_format))\n\u001b[1;32m     <a href='file:///home/yamada/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/torch/autograd/__init__.py?line=51'>52</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/yamada/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/torch/autograd/__init__.py?line=52'>53</a>\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(\u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_border = 0.0001\n",
    "for epoch in range(epoch_): \n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        # X_t_tensor = torch.tensor(data[0], device='cuda:0')\n",
    "        X_t_tensor = data[0].cuda()\n",
    "        # y_t_tensor = torch.tensor(data[1], device='cuda:0')\n",
    "        y_t_tensor = data[1].cuda()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        # print(X_t_tensor.shape)\n",
    "        score = model(X_t_tensor)\n",
    "        loss_ = loss_function(score, y_t_tensor)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "    # if all_loss <= loss_border:\n",
    "    #     print(\"loss was under border(={0}) : train end\".format(loss_border))\n",
    "    #     break\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_history(losses):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    epochs = np.arange(1, len(losses) + 1)\n",
    "\n",
    "    # 損失の推移\n",
    "    ax.set_title(\"Loss\")\n",
    "    ax.plot(epochs, losses)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()\n",
    "plot_history(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0').float()\n",
    "    y_tensor = torch.tensor(y_test, dtype=torch.long, device='cuda:0')\n",
    "            # 推論\n",
    "    y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[1947 1053]\n",
      " [1231 1769]]\n",
      "accuracy =  0.6193333333333333\n",
      "precision =  0.6268603827072998\n",
      "recall =  0.5896666666666667\n",
      "f1 =  0.6076949501889386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('f1 = ', f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/coherence/sbert_coherence_DNN_ver1.pickle\n",
      "success save : ../models/coherence/sbert_coherence_DNN_ver1.pickle\n"
     ]
    }
   ],
   "source": [
    "modelM = DataManager(\"../models/coherence/\")\n",
    "model_name = \"sbert_coherence_DNN_ver{0}.pickle\".format(1)\n",
    "modelM.save_data(model_name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b768359a65658dcda07a01f9fcd77d023c140b301dc2847ae2491657ec52602"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('twitchAI-AEfI_JA6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
