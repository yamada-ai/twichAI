{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"../corpus/novel_formated/\"\n",
    "corpus_name = \"novel_segments.tsv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "conv_data = []\n",
    "with open(out_path+corpus_name, \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    conv_data = [row for row in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1045266"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_data_mini = conv_data[:len(conv_data)//100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10452"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conv_data_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Src_Tgt(conv_data):\n",
    "    src_str = []\n",
    "    tgt_str = []\n",
    "\n",
    "    for conv in conv_data:\n",
    "        src_str.append(conv[-2])\n",
    "        tgt_str.append(conv[-1])\n",
    "    return src_str, tgt_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_str, tgt_str = make_Src_Tgt(conv_data_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# w2v_path = \"../corpus/w2v/\"\n",
    "# w2v_name =  \"model.vec\"\n",
    "# w2v_model = KeyedVectors.load_word2vec_format(w2v_path+w2v_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "def build_vocab(texts):\n",
    "    \n",
    "    counter = Counter()\n",
    "    for text in tqdm(texts):\n",
    "        counter.update( [t.orth_ for t in nlp(text)] )\n",
    "    return Vocab(counter, specials=['<pad>', '<unk>','<fos>', '<eos>', '<del>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:05<00:00, 86.24it/s]\n",
      "100%|██████████| 500/500 [00:05<00:00, 88.29it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_src = build_vocab(src_str[:500])\n",
    "vocab_tgt = build_vocab(tgt_str[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char: <pad>    → Index: 0 \n",
      "Char: <unk>    → Index: 1 \n",
      "Char: <fos>    → Index: 2 \n",
      "Char: <eos>    → Index: 3 \n",
      "Char: <del>    → Index: 4 \n",
      "Char: 、        → Index: 5 \n",
      "Char: !        → Index: 6 \n",
      "Char: 。        → Index: 7 \n",
      "Char: …        → Index: 8 \n",
      "Char: は        → Index: 9 \n",
      "Char: だ        → Index: 10\n",
      "Char: の        → Index: 11\n",
      "Char: ?        → Index: 12\n",
      "Char: て        → Index: 13\n",
      "Char: に        → Index: 14\n"
     ]
    }
   ],
   "source": [
    "for char, index in list(vocab_src.stoi.items())[:15]:\n",
    "    print('Char: {: <8} → Index: {: <2}'.format(char, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD_IDX = vocab_src.stoi[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(texts_src, texts_tgt, vocab_src, vocab_tgt, tokenizer_src, tokenizer_tgt):\n",
    "    \n",
    "    data = []\n",
    "    for (src, tgt) in zip(texts_src, texts_tgt):\n",
    "        src_tensor = torch.tensor(\n",
    "            convert_text_to_indexes(text=src, vocab=vocab_src, tokenizer=tokenizer_src),\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        tgt_tensor = torch.tensor(\n",
    "            convert_text_to_indexes(text=tgt, vocab=vocab_tgt, tokenizer=tokenizer_tgt),\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        data.append((src_tensor, tgt_tensor))\n",
    "        \n",
    "    return data\n",
    "\n",
    "def convert_text_to_indexes(text, vocab):\n",
    "    return [vocab['<fos>']] + [\n",
    "        vocab[token.orth_] for token in nlp(text)\n",
    "    ] + [vocab['<eos>']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_IDX)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "    \n",
    "    def forward(self, sequence):\n",
    "        embedding = self.word_embeddings(sequence)\n",
    "        # Many to Oneなので、第２戻り値を使う\n",
    "        _, state = self.lstm(embedding)\n",
    "        # state = (h, c)\n",
    "        return state"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b768359a65658dcda07a01f9fcd77d023c140b301dc2847ae2491657ec52602"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('twitchAI-AEfI_JA6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
