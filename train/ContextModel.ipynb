{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/share/virtualenvs/twichAI-ydQv36PI/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datatools.analyzer import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "import time\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "from transformer_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"../corpus/novel_formated/\"\n",
    "# corpus_name = \"novel_segments2.tsv\"\n",
    "corpus_name = \"ntt_segment.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "conv_data = []\n",
    "with open(out_path+corpus_name, \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    conv_data = [row for row in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 状況，過去発話が追加された学習データを構築\n",
    "import copy\n",
    "def make_context_added_Src_Tgt(conv_data):\n",
    "\n",
    "    context_src_str = []\n",
    "    tgt_str = []\n",
    "    prev_utt = []\n",
    "    current_situation = [\"\"]\n",
    "    for conv in conv_data:\n",
    "        # 状況が変化したか\n",
    "        if current_situation[0] != conv[0]:\n",
    "            current_situation = conv[:-2]\n",
    "            # エラー対策\n",
    "            if current_situation==[]:\n",
    "                current_situation = [\"\"]\n",
    "            prev_utt = [conv[-2]]\n",
    "        \n",
    "        context_src_str.append([current_situation, copy.deepcopy(prev_utt) ])\n",
    "        prev_utt.append(conv[-1])\n",
    "        tgt_str.append(conv[-1])\n",
    "    \n",
    "    return context_src_str, tgt_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Ntt_src_tgt(conv_data):\n",
    "\n",
    "    context_src_str = []\n",
    "    tgt_str = []\n",
    "    \n",
    "    for conv in conv_data:\n",
    "        context_src_str.append(conv[:-1])\n",
    "        tgt_str.append(conv[-1])\n",
    "        # doc = nlp\n",
    "        # tgt_str.append()\n",
    "    return context_src_str, tgt_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim=5000\n",
    "# src_str, tgt_str = make_context_added_Src_Tgt(conv_data)\n",
    "src_str, tgt_str = make_Ntt_src_tgt(conv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['こんにちは。お元気ですか?', 'はい、元気です。広告代理店での仕事が忙しいですが。'],\n",
       " ['はい、元気です。広告代理店での仕事が忙しいですが。', 'お疲れ様です。私は介護福祉士をしています。'],\n",
       " ['お疲れ様です。私は介護福祉士をしています。', '介護福祉士として働いていらっしゃるんですね。大変なお仕事ですよね。'],\n",
       " ['介護福祉士として働いていらっしゃるんですね。大変なお仕事ですよね。', 'いえ。広告代理店も大変ですよね。因みに私は北海道の一軒家に住んでいます。'],\n",
       " ['いえ。広告代理店も大変ですよね。因みに私は北海道の一軒家に住んでいます。',\n",
       "  '北海道ですか。御飯の美味しい所で羨ましいです。私は青森県出身で山の多い所が好きなので、山の近くに住んでいます']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_str[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_src = mecab_tokenize\n",
    "tokenizer_tgt = mecab_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len=> src_train_val:73397, src_test:18350\n",
      "len=> src_train:66057, src_val:7340\n"
     ]
    }
   ],
   "source": [
    "src_trainval_str, src_test_str, tgt_trainval_str, tgt_test_str= train_test_split(src_str, tgt_str, test_size=0.20, random_state=5)\n",
    "print(\"len=> src_train_val:{0}, src_test:{1}\".format(len(src_trainval_str), len(src_test_str)))\n",
    "\n",
    "src_train_str, src_val_str, tgt_train_str, tgt_val_str= train_test_split(src_trainval_str, tgt_trainval_str, test_size=0.10, random_state=5)\n",
    "\n",
    "print(\"len=> src_train:{0}, src_val:{1}\".format(len(src_train_str), len(src_val_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train_str_ = src_train_str[:lim]\n",
    "tgt_train_str_ = tgt_train_str[:lim]\n",
    "src_val_str_ = src_val_str[:lim//10]\n",
    "tgt_val_str_ = tgt_val_str[:lim//10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tgt_first_sentence(tgt):\n",
    "    tgt_new = []\n",
    "    for t in tqdm(tgt):\n",
    "        sentences = list(nlp(t).sents)\n",
    "        if len(sentences) == 0:\n",
    "            tgt_new.append(t)\n",
    "        else:\n",
    "            tgt_new.append(sentences[0].orth_)\n",
    "\n",
    "    return tgt_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:54<00:00, 91.18it/s] \n",
      "100%|██████████| 500/500 [00:05<00:00, 91.52it/s]\n"
     ]
    }
   ],
   "source": [
    "tgt_train_str_ = tgt_first_sentence(tgt_train_str_)\n",
    "tgt_val_str_ = tgt_first_sentence(tgt_val_str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "def build_vocab(texts, tokenizer):\n",
    "    \n",
    "    counter = Counter()\n",
    "    for text in tqdm(texts):\n",
    "        try:\n",
    "            counter.update(tokenizer(str(text)))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(text)\n",
    "    return Vocab(counter, specials=['<unk>', '<pad>', '<fos>', '<eos>','<sep>', '<cxt>', '<del>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train_str_set = list(set(sum(src_train_str_, [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['難病', 'で', 'ね', '。']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_tgt(\"難病でね。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9717/9717 [00:00<00:00, 18328.41it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 36467.26it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_src = build_vocab(src_train_str_set, tokenizer=tokenizer_src)\n",
    "vocab_tgt = build_vocab(tgt_train_str_, tokenizer=tokenizer_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/vocab/vocab_CModel_src_mini_lim=5000.pickle\n",
      "success save : ../models/vocab/vocab_CModel_src_mini_lim=5000.pickle\n",
      "success save : ../models/vocab/vocab_CModel_tgt_mini_lim=5000.pickle\n",
      "success save : ../models/vocab/vocab_CModel_tgt_mini_lim=5000.pickle\n"
     ]
    }
   ],
   "source": [
    "vocab_path = \"../models/vocab/\"\n",
    "vocab_name = \"vocab_CModel_src_mini_lim={0}.pickle\".format(lim)\n",
    "dictM = DataManager(vocab_path)\n",
    "dictM.save_data(vocab_name, vocab_src)\n",
    "vocab_name = \"vocab_CModel_tgt_mini_lim={0}.pickle\".format(lim)\n",
    "dictM.save_data(vocab_name, vocab_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD_IDX = vocab_src.stoi[\"<pad>\"]\n",
    "PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(texts_src, texts_tgt, vocab_src, vocab_tgt, tokenizer_src, tokenizer_tgt):\n",
    "    \n",
    "    data = []\n",
    "    for (src, tgt) in zip(texts_src, texts_tgt):\n",
    "        src_tensor = torch.tensor(\n",
    "            convert_text_to_indexes_ntt(text=src, vocab=vocab_src, tokenizer=tokenizer_src, mode=\"src\"),\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        tgt_tensor = torch.tensor(\n",
    "            convert_text_to_indexes_ntt(text=tgt, vocab=vocab_tgt, tokenizer=tokenizer_tgt, mode=\"tgt\"),\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        data.append((src_tensor, tgt_tensor))\n",
    "        \n",
    "    return data\n",
    "\n",
    "def convert_text_to_indexes(text, vocab, tokenizer, mode=\"src\"):\n",
    "    if mode==\"src\":\n",
    "        sit = text[0]\n",
    "        segments = [vocab['<sep>']]\n",
    "        for s in sit:\n",
    "            segments += [vocab[token] for token in tokenizer(s.strip(\"\\n\"))] + [vocab['<sep>']]\n",
    "        # 最後消す\n",
    "        segments[-1] = vocab['<cxt>']\n",
    "        utt = text[1]\n",
    "        for u in utt:\n",
    "            segments += [vocab[token] for token in tokenizer(u.strip(\"\\n\"))] + [vocab['<cxt>']]\n",
    "        return segments\n",
    "    # \n",
    "    elif mode==\"tgt\":\n",
    "        return [vocab['<fos>']] + [\n",
    "            vocab[token] for token in tokenizer(text.strip(\"\\n\"))\n",
    "        ] + [vocab['<eos>']]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def convert_text_to_indexes_ntt(text, vocab, tokenizer, mode=\"src\"):\n",
    "    if mode==\"src\":\n",
    "        segments = [vocab['<sep>']]\n",
    "        for u in text:\n",
    "            segments += [vocab[token] for token in tokenizer(u.strip(\"\\n\"))] + [vocab['<sep>']]\n",
    "        segments[-1] = vocab['<cxt>']\n",
    "        return segments\n",
    "    # \n",
    "    elif mode==\"tgt\":\n",
    "        return [vocab['<fos>']] + [\n",
    "            vocab[token] for token in tokenizer(text.strip(\"\\n\"))\n",
    "        ] + [vocab['<eos>']]\n",
    "    else:\n",
    "        return []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = src_train_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_process(\n",
    "    texts_src=src_train_str_, texts_tgt=tgt_train_str,\n",
    "    vocab_src=vocab_src, vocab_tgt=vocab_tgt,\n",
    "    tokenizer_src=tokenizer_src, tokenizer_tgt=tokenizer_tgt\n",
    ")\n",
    "valid_data = data_process(\n",
    "    texts_src=src_val_str_, texts_tgt=tgt_val_str_,\n",
    "    vocab_src=vocab_src, vocab_tgt=vocab_tgt,\n",
    "    tokenizer_src=tokenizer_src, tokenizer_tgt=tokenizer_tgt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "インデックス化された文章\n",
      "Input: tensor([   4,  452,   39, 3440,   22,   19,   16,    7,   33,   12,  232,   22,\n",
      "          36,    8,  281,   29, 1855,   20,   15,   17,   22,    7,    4,   32,\n",
      "          52,   15,   17,   11,   26,    7,   33, 1080,  397,   23,  145,  156,\n",
      "          17,   11,   36,  141, 1103,   13, 2717,   11,   26,   21,    5])\n",
      "Output: tensor([   2, 4903,   27,   10,    7,  846,    8, 2838,   27,    0,   15,    8,\n",
      "           0,   13, 3966,   57,   27,  139,   16,    0,   17,    7,    3])\n",
      "\n",
      "インデックス化された文章をもとに戻す\n",
      "Input: <sep> 歯医者 って 恐怖 だ よ ね 。 私 の 話 だ けど 、 夫 と 死別 し た ん だ 。 <sep> そう だっ た ん です か 。 私 医療 関係 で 働い でる ん です けど ご 病気 が 原因 です か ? <cxt>\n",
      "Output: <fos> 難病 で ね 。 ま 、 バスケットボール部 で <unk> た 、 <unk> の 明る さ で 前 に <unk> よ 。 <eos>\n"
     ]
    }
   ],
   "source": [
    "print('インデックス化された文章')\n",
    "print('Input: {}\\nOutput: {}'.format(train_data[0][0], train_data[0][1]))\n",
    "print('')\n",
    "\n",
    "print('インデックス化された文章をもとに戻す')\n",
    "print('Input: {}\\nOutput: {}'.format(\n",
    "    ' '.join([vocab_src.itos[x] for x in train_data[0][0]]),\n",
    "    ' '.join([vocab_tgt.itos[x] for x in train_data[0][1]])\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "# batch_size = 48\n",
    "PAD_IDX = vocab_src['<pad>']\n",
    "START_IDX = vocab_src['<fos>']\n",
    "END_IDX = vocab_src['<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "    \n",
    "    batch_src, batch_tgt = [], []\n",
    "    for src, tgt in data_batch:\n",
    "        batch_src.append(src)\n",
    "        batch_tgt.append(tgt)\n",
    "        \n",
    "    # batch_src = pad_sequence(batch_src, padding_value=PAD_IDX, batch_first=True)\n",
    "    # batch_tgt = pad_sequence(batch_tgt, padding_value=PAD_IDX, batch_first=True)\n",
    "    batch_src = pad_sequence(batch_src, padding_value=PAD_IDX, batch_first=False)\n",
    "    batch_tgt = pad_sequence(batch_tgt, padding_value=PAD_IDX, batch_first=False)\n",
    "    \n",
    "    return batch_src, batch_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['そうなんですね。',\n",
       " 'うーん。',\n",
       " 'そうなんだ。',\n",
       " '夏じゃなく、春の気候のいい頃に帰ってみては?',\n",
       " 'その人にとっては、嬉しい物だったんだよ。',\n",
       " 'そうなんですよ、情勢が不安定だったりしますからね。',\n",
       " 'ごもっともですね。',\n",
       " '子沢山のお家だったんですか。',\n",
       " 'そうですか、私は翻訳家をしていますが、あなたはどんな仕事をしていますか?',\n",
       " '銀行員は柵が多そうですね?',\n",
       " 'お互い大変だよね。',\n",
       " 'そっか、じゃあ何か楽しい話をしようよ。',\n",
       " 'わたしはまだ大学院生で、バイトで家庭教師はしています。',\n",
       " '商品を返品した方がいいよ。',\n",
       " 'ラグビーです。',\n",
       " '納豆嫌いなんだね。',\n",
       " 'わたしは絵を描くのが好きなんです。',\n",
       " '我慢強いから、もう少し頑張ってみるよ。',\n",
       " 'すばらしいですね。',\n",
       " 'そんな深刻にならないで。',\n",
       " 'わたしは要領が悪いだけで時間がないけどうまくやればできると思うよ。',\n",
       " '辰年です。',\n",
       " 'はい、効果があるという口コミが多いので期待しています。',\n",
       " '高級チョコレートだったんだよ!',\n",
       " 'でしょー!',\n",
       " 'もちろん、わざとではないですが、娘が大事にしていた玩具なので、うしろめたさを感じます。',\n",
       " 'いい夫婦の日なんですね。',\n",
       " '確かに急なリストラは、日本の会社ではあまりないよね。',\n",
       " '学生さんかぁ。',\n",
       " '折れにくい上に、書きやすいし、消しやすいし、お値段もそんなに高くないから御勧めだよ!',\n",
       " '幼稚園の先生してるよ。',\n",
       " '20人くらいかな。',\n",
       " 'もちろんです!頑張ってください!',\n",
       " 'わたしは専業主婦なので、悠々自適の生活なんですよ。',\n",
       " 'もうそんな大きくなっちゃったの?',\n",
       " 'うん。',\n",
       " '誕生日が近かったので、わたし宛てのプレゼントかなって期待してしまいました。',\n",
       " 'そんなに混んでるんだね。',\n",
       " '僧侶さんでしたか!',\n",
       " 'いいなぁ。',\n",
       " 'まずは記帳してみましょう',\n",
       " 'まあまあ、汚れに気づいた時に掃除すれば大丈夫、大丈夫。',\n",
       " 'そうでしたか、それはご愁傷様です。',\n",
       " 'マナーは守ってほしいですね。',\n",
       " 'そうなんですか、外国の方だと気付かなかったんです。',\n",
       " 'そうですよねー!',\n",
       " 'うん!',\n",
       " '私は今日は久し振りに一人で羽根を伸ばすわ。',\n",
       " 'うん!',\n",
       " 'きっとまた好みの車が見つかるよ。',\n",
       " 'えー、幸せオーラ全開じゃん!',\n",
       " 'うん。',\n",
       " '色々と挑戦するのですが、その挑戦が多岐にわたっていて、しかもどれを見ても面白いから無差別に見てしまいます。',\n",
       " 'そうなの。',\n",
       " 'その友達はピアノも上手だし、フルートの音を聞きながら演奏してくれるから信頼しているんだ。',\n",
       " 'そうみたいです。',\n",
       " 'へえ、話題ですよね。',\n",
       " '子沢山だったのは私で、今子供二人と暮らしていますよ。',\n",
       " '本当に。',\n",
       " 'それはひどいなあ、わたし応援団だったので、励ましてあげますよ。',\n",
       " 'アメリカです。',\n",
       " '本当にそうです。',\n",
       " '私も独身貴族を謳歌しています!',\n",
       " 'そうなんですね。',\n",
       " 'そうですね、でも慣れました。',\n",
       " '専門家でもそうなんだ。',\n",
       " '格好いいですね。',\n",
       " 'きっと大丈夫ですよ!',\n",
       " '置いてあった場所がすっきりしてて寂しいな、また何か置こうかな?',\n",
       " 'そんなに違うんだ。',\n",
       " '気が利きますね。',\n",
       " '机の引き出しにしまってあった。',\n",
       " 'わたしは商社マンなんだけど、大きい商談がまとまった時とかかな?',\n",
       " '何かきっちりやめられる方法がないのか、知りたいです。',\n",
       " '意外とそうなんです。',\n",
       " 'のんびりした性格って人に良く言われるんです。',\n",
       " 'お肌がつるつるしてそうですね。',\n",
       " 'しっかり、というほどではないよ。',\n",
       " '本当だね。',\n",
       " '彼氏と母親が仲が良すぎるので、何かあった時に母親の肩を持ちそうでちょっと心配なんです。',\n",
       " 'ちょっと九州まで。',\n",
       " 'そうだったんですか。',\n",
       " 'もしかしたら誰かに盗られたかもと思った自分が恥ずかしくて、落ち込む',\n",
       " '部活の練習が厳しいのは我慢できるけど、人間関係が厳しすぎたら私も辞めちゃうかもしれないわ!',\n",
       " '御土産代も節約したいので皆に内緒にして、体調不良で休むって言っているのが少しうしろめたいです。',\n",
       " 'メーキャップアーティストっていうと、芸能人のメークをしたりする奴?',\n",
       " 'そう、付き合い始めの記念日。',\n",
       " 'あら、じゃあ、東京じゃなくて神戸にきなさいよ。',\n",
       " '勇気を出して歯医者に行ってみるね。',\n",
       " 'え!',\n",
       " 'わたしもまだよくわからないんだけど、鍾乳洞かな?',\n",
       " 'そうだよね。',\n",
       " '気配も足音も消せる上司さんって、何者なのかしら?',\n",
       " '大切な人から貰ったの?',\n",
       " 'やっぱり生まれ育った場所って、性格にあるていど影響するよね。',\n",
       " '大変そうな仕事だね。',\n",
       " '30代になって初めてこの前同窓会をしました!',\n",
       " 'そうだね。',\n",
       " 'キャリアコンサルタントっていって、人事関係の相談に乗るお仕事なんです。',\n",
       " 'わたしは団地暮らしなんですがさすがに隣人の子が泣き出すと集中も切れますけどね。',\n",
       " 'そうですか。',\n",
       " 'なるほど、そうだったんですね。',\n",
       " 'いいね、食べたい、美味しいお米になるまですごい大変なんだよね、本当感謝しかないよ。',\n",
       " '5年くらい。',\n",
       " 'ああ、やっぱり、どこの弟も自由でうらやましいな!',\n",
       " 'そっか。',\n",
       " 'まあ、味は変わらないと思いますよ。',\n",
       " 'うん。',\n",
       " '同じ本がすぐ見つかるといいね。',\n",
       " '人参は私も馬も好きですよ。',\n",
       " '全部良しって訳にはいかないんですね。',\n",
       " '守りたくなる感じで可愛らしいですね!',\n",
       " 'ぜひ探してみてください。',\n",
       " 'それは安心ですね!',\n",
       " 'うん、そっとしておいた方が良いね。',\n",
       " 'そう、せっかく集中して見ていたのに。',\n",
       " '友達は調理師免許が取れるコースにいるから、そこ主催のレストランに行くのがとても楽しみなの。',\n",
       " 'そうですか、私は顔立ちが幼いので、ショートにしてしまうと、より子供っぽく見られてしまうんですよね。',\n",
       " 'どうやらそうみたい。',\n",
       " '1位だよ!',\n",
       " '今回の方法はたくさんの人の花粉症を治してきた、とても画期的な物だそうです。',\n",
       " '独立かぁー、大変そうだけど、好きなことを仕事にできるのっていいよねー!',\n",
       " '私は母子家庭で一人の時間が多かったので本を読むのが好きです!',\n",
       " 'わかる、わかる。',\n",
       " 'そうなんですか?',\n",
       " '趣味と言えば釣りぐらいな物で家にこもりがちでしたから、楽しみが出来ました。',\n",
       " '大きい夢があって、眩しいわ!',\n",
       " 'それ最高の具材ですね。',\n",
       " 'それはありがたかったですね。',\n",
       " 'そうなんですね。',\n",
       " '童心に帰りますね。',\n",
       " 'そうですか。',\n",
       " '見かねた旦那が、夕飯を準備して待っててくれるんだよね。',\n",
       " 'そうですね。',\n",
       " '息子さんのことを1番に考えて行動してね',\n",
       " '日本人も世界に通用するようになったんだなあ、って思いますね。',\n",
       " 'あ、同じなんですね!',\n",
       " '今年はコロナも流行ってるから、インフルエンザと同時にかからないかどうかとっても心配なんですよ。',\n",
       " 'それ、旦那の口癖!',\n",
       " 'そういう問題かよ!',\n",
       " '妊娠中は仕方がないですよ。',\n",
       " '後でわかるより、先に話した方がいいと思うよ。',\n",
       " 'はい。',\n",
       " 'その特集記事、読んでみたいな。',\n",
       " 'わたし青森生まれなんで、けっこう寒いのは強い方なんですけど、今年の冬はちょっと異常です。',\n",
       " 'なるほど。',\n",
       " '帰ってくるのが楽しみだね。',\n",
       " 'まだ独身なんで、友達だったり、一人だったりです。',\n",
       " 'たかが階段、されど階段だね。',\n",
       " 'はい!',\n",
       " 'ぜひ。',\n",
       " 'そうですかね。',\n",
       " '母はわたしのことを全部わかっているので、母の紹介に間違いはないと信頼しています。',\n",
       " '京都美人か!',\n",
       " 'すっごく密にあります!',\n",
       " 'ちょっといい男なんですよね、今日の御相手。',\n",
       " 'はい。',\n",
       " 'それで、夜道を歩いてたら、酔っ払いが急に絡んできたの!',\n",
       " 'そうです。',\n",
       " '家では香水つけないので大丈夫ですよ!',\n",
       " '独身です!',\n",
       " '退場処分にはならなかったのですか?',\n",
       " 'それは恥ずかしいだろうな。',\n",
       " '心配はあまりしていませんでしたが、いざ独り暮らしが始まると子供が一人いないことで家の中がとても静かになりました。',\n",
       " 'ありがと。',\n",
       " 'ははは。',\n",
       " '是非、今度、こつを教えて下さい!',\n",
       " 'もう会えなくなるのはさみしいね。',\n",
       " 'そうなんですか、ソーシャルワーカーって、どんなことをするんですか?',\n",
       " '怒らせた覚えもないし何かあったのかな、不安だよ。',\n",
       " '美味しそうですね。',\n",
       " '茶髪のことでかあ。',\n",
       " 'これからも、頑張ってくださいね。',\n",
       " '手間も時間もかかるけど、細かい作業が好きだからいい息抜きにもなってるよ。',\n",
       " '実にご立派なご職業かと存じます。',\n",
       " 'えらいですね。',\n",
       " 'ああ、では今度御店に来てください。',\n",
       " 'まさにそれなの。',\n",
       " 'それが私だけ、声をかけてもらえなくて、今悲しい気持ちです。',\n",
       " 'ストレス発散は大切ですよね。',\n",
       " 'やっぱり声を出すのはストレス発散に繋がるしね。',\n",
       " 'うれしいね。',\n",
       " '家のかぎのスペアはありますが、かぎが見つからないのでもし悪意ある人が拾っていたらと怖くなります。',\n",
       " 'ですね。',\n",
       " 'ご実家にお住まいなんですね。',\n",
       " '仕事ねー。',\n",
       " 'ファッションデザイナーになりたいって、夢だけはあるんですよ!',\n",
       " '栄養バランスも考えていっぱい御数をいれてくれるから、毎日感謝しているよ。',\n",
       " 'そうなんだ、次はどんな仕事を探しているの?',\n",
       " 'ジョブズの伝記とかを読みますね。',\n",
       " '断っても断っても、また別の所から来るから嫌になるよ。',\n",
       " 'そうですねーファッションセンスがきっと、あるんでしょうね。',\n",
       " 'ははは。',\n",
       " 'いつも信頼してお任せしているんですよ。',\n",
       " 'いいですよ!',\n",
       " 'それは残念だね。',\n",
       " 'あはは。',\n",
       " 'ありがとうございます。',\n",
       " '細かい手作業という意味では似ているかもしれませんね。',\n",
       " 'こちらの話をよく聞いてくれて、質問にも丁寧に答えてくれます。',\n",
       " 'ううん。',\n",
       " '冷凍食品の進化は目を瞠る物がありますね。',\n",
       " '染み抜きの漂白剤と、日焼け止め',\n",
       " 'まだ新婚だからね。',\n",
       " '内緒で食べるのは罪の味ですね。',\n",
       " 'はい。',\n",
       " 'へー、そんなに優秀なんだ。',\n",
       " 'そうなの。',\n",
       " 'そうなる可能性まで考慮するのが、大人ってもんよ。',\n",
       " '今度試してみます!',\n",
       " 'そうなんですね。',\n",
       " 'そんなに心配なら、急いで送った方がいいと思うよ。',\n",
       " 'ええ、あちこちに。',\n",
       " '修学旅行かー。',\n",
       " 'そうなんだ!',\n",
       " 'そうなんですか。',\n",
       " 'もともと問題視されてる人だったので、人事もちょっとだけ動いてくれました。',\n",
       " 'はい!',\n",
       " '整体師さんでしたか!',\n",
       " 'ぽっちゃりの方が好きな人もいますけどね。',\n",
       " 'ありがとう!',\n",
       " '大阪市で3位になったことがあります。',\n",
       " 'だといいんですけど!',\n",
       " '結構怖かった。',\n",
       " 'お料理上手な御母様で羨ましい。',\n",
       " '同じことが起こらないように指導していかなきゃいけませんね。',\n",
       " 'そうなんですね!',\n",
       " 'ああ、それは絶対上がる気がするよ!',\n",
       " '連れて行ってあげたんだ!',\n",
       " '子供の成長って早いねー!',\n",
       " '何度でもチャンスはあるのでまた学校行きたくなったらトライしてみてください。',\n",
       " '技術の進歩はすごいですよね。',\n",
       " 'わたしは手羽先や天むすもオススメしたいな。',\n",
       " '素敵な彼氏だね。',\n",
       " '移住ですか!',\n",
       " '名古屋飯いいな!',\n",
       " '獣医さんなんてすごいね。',\n",
       " '何年もアドバイザーをしている人だから、意見が参考になってとても信頼しているよ。',\n",
       " '思い切り汗かくのも気持ちいですよね!',\n",
       " 'そうなのね。',\n",
       " '行きます行きます!',\n",
       " '本当にありがたいですよね。',\n",
       " '頑張て下さいね。',\n",
       " '子供は何でも吸収しちゃうからこの侭いくとどうなっちゃうのか、止めると益々面白がって使うし、母としては心配です',\n",
       " '飛行機代って高いからね。',\n",
       " 'わたしはスポーツ好きだけどあなたは他に好きなことがあるんだね。',\n",
       " '怖い。',\n",
       " '収入も比例して上がってくれれば良いのにねー。',\n",
       " 'ああ、ますますお金が貯まりそうですね!',\n",
       " 'とてもしっかりした部下がいるので、彼に届けてもらう積もりです。',\n",
       " '動物がお好きなんですね。',\n",
       " '3社目で希望していた販売の会社に内定しました。',\n",
       " '家賃が安く済むというのは大きいですよね。',\n",
       " '好き嫌いしていると背が伸びないよ。',\n",
       " '私、暑いのが本当に苦手なんです。',\n",
       " '皆で御飯作ったり食べたりするの楽しいですよね。',\n",
       " '動物を飼うのはどう?',\n",
       " '時間が出来たらこっちに遊びに来てください。',\n",
       " '成長にグッとくるのかね',\n",
       " 'あるある!',\n",
       " '嬉しいです!',\n",
       " 'そうなんですね。',\n",
       " 'そう思うんですけどね、私は運動全般が苦手なんですよ。',\n",
       " 'いえいえ。',\n",
       " 'そうだったんですね、離島生まれならなおさらでしょう。',\n",
       " '頑張れ!応援してるよ。',\n",
       " 'まだ会ったことないんだけど、隣のベランダに大量のビール缶があるの。',\n",
       " '次から頑張れ。',\n",
       " 'そうなんです。',\n",
       " 'えへへ。',\n",
       " 'なるほど。',\n",
       " 'hiphopです',\n",
       " '自信のある演奏だったので、もう無理かなって感じました。',\n",
       " '心震えますね。',\n",
       " '少しのうっかりでこうなるなんて、ショックだよ。',\n",
       " '今からドキドキしているのですが、とにかく素直な気持ちをぶつけてこようと思ってます。',\n",
       " 'それなら、一緒に食事を楽しめるパートナーを探しましょう!',\n",
       " '結婚式はお金使うよね。',\n",
       " 'その男性から彼女がいますと打ち明けられました。',\n",
       " 'そんなことあるんだね。',\n",
       " '消防士なんです。',\n",
       " '大変ですね。',\n",
       " '私の教え子をご紹介しましょうか?',\n",
       " 'はい。',\n",
       " '環境が人を育むって言いますよね。',\n",
       " '血液型と誕生日のどちらで占いましょうか?',\n",
       " '美味しいんですね。',\n",
       " '本屋さんてついつい長居しちゃいませんか?',\n",
       " 'はい、千葉辺りまで。',\n",
       " '最近はコンビニスイーツもおいしいですからね。',\n",
       " '知り合いの紹介でさ。',\n",
       " '至福の時ですね。',\n",
       " 'うん。',\n",
       " '映画鑑賞が趣味なんですね。',\n",
       " 'そうなんですね。',\n",
       " 'わかるわかる!',\n",
       " '寝れないって言ってて、病院に行って薬もらったっては訊いてたけど、それってずい分前なんだよね、それが今も飲んでるってなると心配だよ。',\n",
       " 'それならよかったです。',\n",
       " 'へえ、それは頼もしい!',\n",
       " '本当ですか?',\n",
       " 'ああ、あれですよね、子供を持たない主義の夫婦!',\n",
       " 'それがよさそうだね。',\n",
       " 'お年寄りが多いからな。',\n",
       " 'そんなこと言わないで。',\n",
       " 'わたしもー。',\n",
       " 'ううん、中津空揚げを食べに行ったんだ。',\n",
       " 'はいはい、わたしも仕事をしていた頃は体調管理に気を遣いました。',\n",
       " '一緒に遊びを楽しめるのがいいですよね!',\n",
       " '独身です。',\n",
       " 'その人無事だと良いですね。',\n",
       " 'そうそう!',\n",
       " 'そうですか、オセロも得意なんですが、今度やりませんか?',\n",
       " '木版かー、今?',\n",
       " '海育ちってことか。',\n",
       " '毎年同じ場所に巣を作りに来ます。',\n",
       " '私は青森で生まれ今も山のそばに住んでいて、いつも郭公の鳴き声が聞こえます。',\n",
       " 'そうね!',\n",
       " 'うん、どっちにするにしても、慎重にね!',\n",
       " 'ええ、そうなんです。',\n",
       " '娘さんの成長が嬉しかったんですね。',\n",
       " 'いいですよ。',\n",
       " 'また次の仕事で頑張りましょう。',\n",
       " '農家なので、黙々と仕事をこなすことが多いんです。',\n",
       " 'あら。',\n",
       " 'お値段はサービスしときますから、リフレッシュしにぜひ来てくださいね。',\n",
       " 'おー、それは縁の良さそうな誕生日ですね。',\n",
       " '同じく私も料理は全然できないよ。',\n",
       " 'それは気が合いそうだね。',\n",
       " '美容院に行く時間もないほど、忙しいの?',\n",
       " 'お好きなようにカスタマイズしますよ!',\n",
       " '坂本竜馬が好きなんだね!',\n",
       " '逼迫していますね。',\n",
       " 'これに凝りて気をつけてくれると良いんですけどねぇ。',\n",
       " '頑張って!',\n",
       " '丸刈りなんですか!',\n",
       " '不安なら、今からでも買いに行っては?',\n",
       " 'ああ、仙台とかあっちの方ね。',\n",
       " 'とても上手なのは知ってましたが、それほどとは思いませんでした。',\n",
       " 'すっかりお姉ちゃん気分で手伝ってくれて、楽しかったよ。',\n",
       " 'へぇ、あの壁とか崖を登る競技ですよね。',\n",
       " 'そうかもしれませんね。',\n",
       " 'いつも、お花見後のゴミがひどいんだって。',\n",
       " '修理しようか、でも10年経つから買い換えるかって考えていたら、どんどん洗濯物が溜まってしまって。',\n",
       " 'でしょー!',\n",
       " '肌の色が濃いから美白することにハマってるよ。',\n",
       " 'そこまでして飲み会を開くんですね。',\n",
       " '蜜柑おいしいですよ。',\n",
       " 'ハムスターなんてとってもかわいいですね!',\n",
       " 'それは大変。',\n",
       " '何とかね。',\n",
       " 'どこまでが本当なんだか!',\n",
       " '近くのお寺でやるよ。',\n",
       " 'そっか。',\n",
       " '地元の強みですね。',\n",
       " 'とても格好よい、自慢のお子さんですね。',\n",
       " '御菓子って、栄養偏っちゃうから、やっぱり制限しないと駄目だよね。',\n",
       " '寒いのに、今日も患者さんがたくさん来ましたよ。',\n",
       " 'ちょっと、それお母さんに言ってくるよ!',\n",
       " '植木鉢を玄関に入れたり、自転車を倒しておいたりしました。',\n",
       " 'かっこいいですね。',\n",
       " 'いいですね、がんばってください。',\n",
       " '色んな相談にも乗ってくれてわたしの母みたいな人だったので、卒園して会えなくなるのがさびしいですね。',\n",
       " '普段から少しは洋服きてた方がいいよ。',\n",
       " 'かっこいいですね。',\n",
       " '新聞社勤めは皆どこか不健康ですね。',\n",
       " 'そうしましょう、そうしましょう!',\n",
       " '足を骨折して入院しました。',\n",
       " 'もう一回隅々までよく探してみてください。',\n",
       " '私にも教えてください。',\n",
       " 'そうなんだ。',\n",
       " '尊敬できますね。',\n",
       " 'それは、羨ましいですね。',\n",
       " 'ひどすぎます!',\n",
       " '気持ちの良い朝ですね。',\n",
       " 'ぜひ。',\n",
       " '何もしてないよ!',\n",
       " 'そうなんだ!',\n",
       " '私は兼業主婦です。',\n",
       " 'はい。',\n",
       " 'ゴルフもいいなー!',\n",
       " 'そうなのですね。',\n",
       " 'おならは生理現象だから、出てしまっても仕方ないよ。',\n",
       " 'そうなんですね。',\n",
       " '母が、歯の健康はずっと気にしてくれていて、助かったよ。',\n",
       " '最近全国各地で被害が大きいし、何か対策しておくべきだね。',\n",
       " 'それは、大変だったね。',\n",
       " 'どん引きです。',\n",
       " 'そうだよ!',\n",
       " '本当ですか?',\n",
       " '大変ですよ。',\n",
       " '起こると思いますよ。',\n",
       " 'それが出先のカフェだったから、ダスターでポンポンして、急いで帰って洗ったけど薄っすら色が付いたかも、落ち込むわ',\n",
       " 'そうですね、お互い頑張りましょうね。',\n",
       " '電機メーカーに勤めてたんだ!',\n",
       " 'それは嫌な情報ですね。',\n",
       " '努力したんだろうね。',\n",
       " '今度からもうずるは駄目だよ。',\n",
       " '便利な世の中ですね',\n",
       " 'それで電機メーカーに就職したんだ。',\n",
       " 'ええ、陰ながら応援しますので、がんばってくださいね。',\n",
       " 'いいことしたね。',\n",
       " 'それは場所が悪かったね。',\n",
       " '是非来てみて下さい!',\n",
       " 'もう、プロだね。',\n",
       " '早足で歩いたのですが、後ろの人も早足になって生きた心地がしなかったです。',\n",
       " '大当たり!',\n",
       " 'ぜひ!',\n",
       " '旦那さんにとっても宝物だと思いますよ。',\n",
       " '櫃塗しの有名な名古屋ですか!',\n",
       " '私も転職したことないです、憧れている芸能人がいるのでずっとこの業界です。',\n",
       " '高校球児の時は、肉を沢山食べさせてもらえたんだけどね。',\n",
       " '住んでいたのですね。',\n",
       " 'よくわからないけど、とにかく高かった!',\n",
       " '我慢強いと溜め込むのは紙一重だから気をつけてね!',\n",
       " '食事、楽しんできてね。',\n",
       " '勉強を楽しく感じるのは、いい傾向だね。',\n",
       " '空港でしたら、今の御時世大変そうですね。',\n",
       " 'そっか。',\n",
       " '私も近いよ!',\n",
       " '夕飯を作る予定なので、今から献立を考えています。',\n",
       " 'うん!',\n",
       " 'ありがとうございます。',\n",
       " 'それは心配だね、私はブライダル関係の会社だから仕事が減っちゃって。',\n",
       " 'そうなんだね。',\n",
       " 'うんうん。',\n",
       " '私は農家なので、春になればまた忙しくなりますけどね。',\n",
       " 'それは意外です、役所のイメージが変わりますね。',\n",
       " '良いよ!',\n",
       " '楽しそうな趣味ですね。',\n",
       " '平日でも全然OK!',\n",
       " '夫と行くんだけど、途中で、はぐれないないかな。',\n",
       " '就職難の中、おめでとうございます!',\n",
       " 'はい、しかも寝ている間に突然雷の音がしたのでがたがた震えてしまいました。',\n",
       " 'そんなに取れない物なのかー。',\n",
       " 'うん!',\n",
       " 'それは良いですね!',\n",
       " 'そんなに良かったんだ。',\n",
       " 'なるほど。',\n",
       " '何か手伝えることはありませんか?',\n",
       " 'わぁ、近いですね。',\n",
       " 'はい、もう5年になります。',\n",
       " 'そうかもしれないですね。',\n",
       " 'そうなんです。',\n",
       " '握り拳大ってスゴイね!',\n",
       " 'さっそく焼いて食べたら、最高の香りでした。',\n",
       " 'それがね、驚くことに乾いてたの!',\n",
       " 'そうなんですか?',\n",
       " '絵が趣味なんだね。',\n",
       " '仕事は東京の会社に勤めてます。',\n",
       " 'すてきですね、いつか会えるといいですねー。',\n",
       " 'そうなんですよ。',\n",
       " '郵便局に連絡するのも手間ですからね。',\n",
       " '休みの日は育児ですよ!',\n",
       " '近所をふらふらしてます、繁華街に住んでいるのでそれなりに楽しいです。',\n",
       " 'そうだね。',\n",
       " '蛇年生まれだけど蛇は苦手で兎が好きです。',\n",
       " 'うん!',\n",
       " '猫背が原因?',\n",
       " 'それはバザーの日が、待ち遠しいね。',\n",
       " '全然そんなことなかったと記憶しています。',\n",
       " 'それが、僕が書いた物じゃ無いんだ。',\n",
       " 'ずっと都会育ちなの?',\n",
       " 'えー!',\n",
       " 'そうですね。',\n",
       " '負けたことが良い経験になると期待するしかありませんね。',\n",
       " '仕事自体は好きなので、ゆくゆくは戻りたいと考えています',\n",
       " '泳ぐのは苦手なんです。',\n",
       " '狙ってそれっぽくしたらしいんだけど、はまりすぎだよ。',\n",
       " '持ってた持ってたー!',\n",
       " 'プラモデルですか!',\n",
       " 'はい、未だに手をつないで歩いていました。',\n",
       " 'そうなんだね!',\n",
       " '自動車整備士ですよ。',\n",
       " 'それは、大変でしたね!',\n",
       " 'ラーメンばっか食ってますね。',\n",
       " 'なるなる!',\n",
       " '続けてきて偉かったね、誇りに思うよって言いました。',\n",
       " '命の誕生に携わる、大事なお仕事なんだね。',\n",
       " '怪我のないように、がんばってね。',\n",
       " '温泉いいですね。',\n",
       " 'うん。',\n",
       " '伴走者と一緒にゴールする姿には深い感銘を受けました。',\n",
       " '結構エキサイトしていたので恥ずかしいです。',\n",
       " '全然素敵じゃないよ。',\n",
       " '弟さんも、良い所見せたいんだろうね。',\n",
       " '責任のあるお仕事をされているんですね。',\n",
       " 'そう、だから彼に嫌いになったから結婚できないって、はっきり言ったんだ。',\n",
       " 'あはは。',\n",
       " '新しくパイロットになった人だよとても優しいの。',\n",
       " '自業自得でしょ。',\n",
       " 'ありがとう。',\n",
       " 'いいなぁ。',\n",
       " 'あまり無理しなくて良いと思うよ!',\n",
       " '京都でも洋服が主流ですよー。',\n",
       " '新年ですしね。',\n",
       " 'それは誰でも焦るよ!',\n",
       " '帰りは下着を履いていないのを内緒にして、帰りました。',\n",
       " 'わーい!',\n",
       " 'いいですね。',\n",
       " 'そうですね、またお教えしますね。']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_val_str_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, PAD_IDX):\n",
    "    \n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for src, tgt in tqdm(data):\n",
    "        \n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        input_tgt = tgt[:-1, :]\n",
    "\n",
    "        mask_src, mask_tgt, padding_mask_src, padding_mask_tgt = create_mask(src, input_tgt, PAD_IDX)\n",
    "\n",
    "        # print(src.shape, tgt.shape)\n",
    "\n",
    "        logits = model(\n",
    "            src=src, tgt=input_tgt,\n",
    "            mask_src=mask_src, mask_tgt=mask_tgt,\n",
    "            padding_mask_src=padding_mask_src, padding_mask_tgt=padding_mask_tgt,\n",
    "            memory_key_padding_mask=padding_mask_src\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_tgt = tgt[1:, :]\n",
    "        loss = criterion(logits.reshape(-1, logits.shape[-1]), output_tgt.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "        del logits\n",
    "        del loss\n",
    "        \n",
    "    return losses / len(data)\n",
    "\n",
    "\n",
    "def evaluate(model, data, criterion, PAD_IDX):\n",
    "    \n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    for src, tgt in data:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        input_tgt = tgt[:-1, :]\n",
    "\n",
    "        mask_src, mask_tgt, padding_mask_src, padding_mask_tgt = create_mask(src, input_tgt, PAD_IDX)\n",
    "\n",
    "        logits = model(\n",
    "            src=src, tgt=input_tgt,\n",
    "            mask_src=mask_src, mask_tgt=mask_tgt,\n",
    "            padding_mask_src=padding_mask_src, padding_mask_tgt=padding_mask_tgt,\n",
    "            memory_key_padding_mask=padding_mask_src\n",
    "        )\n",
    "        \n",
    "        output_tgt = tgt[1:, :]\n",
    "        loss = criterion(logits.reshape(-1, logits.shape[-1]), output_tgt.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "        del logits\n",
    "        del loss\n",
    "        \n",
    "    return losses / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_src = len(vocab_src)\n",
    "vocab_size_tgt = len(vocab_tgt)\n",
    "embedding_size = 320\n",
    "nhead = 8\n",
    "dim_feedforward = 100\n",
    "num_encoder_layers = 4\n",
    "num_decoder_layers = 4\n",
    "dropout = 0.1\n",
    "\n",
    "model = Seq2SeqTransformer(\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    embedding_size=embedding_size,\n",
    "    vocab_size_src=vocab_size_src, vocab_size_tgt=vocab_size_tgt,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout, nhead=nhead,\n",
    "    PAD_IDX=PAD_IDX\n",
    ")\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# lr = 0.05\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:08<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/30] train loss: 5.95, valid loss: 5.13  [8s] count: 0, **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/30] train loss: 5.53, valid loss: 5.11  [8s] count: 1, **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/30] train loss: 5.53, valid loss: 5.12  [8s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/30] train loss: 5.52, valid loss: 5.12  [8s] count: 2, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/30] train loss: 5.52, valid loss: 5.14  [8s] count: 3, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:08<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/30] train loss: 5.50, valid loss: 5.44  [8s] count: 4, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:08<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/30] train loss: 5.46, valid loss: 6.07  [8s] count: 5, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:08<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/30] train loss: 5.44, valid loss: 6.22  [9s] count: 6, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/30] train loss: 5.43, valid loss: 6.11  [8s] count: 7, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:08<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/30] train loss: 5.42, valid loss: 6.36  [8s] count: 8, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:08<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/30] train loss: 5.41, valid loss: 5.94  [8s] count: 9, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:08<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/30] train loss: 5.41, valid loss: 5.86  [8s] count: 10, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:08<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/30] train loss: 5.41, valid loss: 6.46  [8s] count: 11, \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "epoch = 30\n",
    "best_loss = float('Inf')\n",
    "best_model = None\n",
    "patience = 10\n",
    "counter = 0\n",
    "\n",
    "for loop in range(1, epoch + 1):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    loss_train = train(\n",
    "        model=model, data=train_iter, optimizer=optimizer,\n",
    "        criterion=criterion, PAD_IDX=PAD_IDX\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    loss_valid = evaluate(\n",
    "        model=model, data=valid_iter, criterion=criterion, PAD_IDX=PAD_IDX\n",
    "    )\n",
    "    \n",
    "    print('[{}/{}] train loss: {:.2f}, valid loss: {:.2f}  [{}{:.0f}s] count: {}, {}'.format(\n",
    "        loop, epoch,\n",
    "        loss_train, loss_valid,\n",
    "        str(int(math.floor(elapsed_time / 60))) + 'm' if math.floor(elapsed_time / 60) > 0 else '',\n",
    "        elapsed_time % 60,\n",
    "        counter,\n",
    "        '**' if best_loss > loss_valid else ''\n",
    "    ))\n",
    "    \n",
    "    if best_loss > loss_valid:\n",
    "        best_loss = loss_valid\n",
    "        best_model = model\n",
    "        counter = 0\n",
    "        \n",
    "    if counter > patience:\n",
    "        break\n",
    "    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/transformer/CModel_lim=5000_best.pickle\n",
      "success save : ../models/transformer/CModel_lim=5000_best.pickle\n",
      "success save : ../models/transformer/CModel_lim=5000.pickle\n",
      "success save : ../models/transformer/CModel_lim=5000.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/transformer/\"\n",
    "model_name = \"CModel_lim={0}_best.pickle\".format(lim)\n",
    "modelM = DataManager(model_path)\n",
    "modelM.save_data(model_name, best_model)\n",
    "model_name = \"CModel_lim={0}.pickle\".format(lim)\n",
    "modelM.save_data(model_name, model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6fe7309c4cab71bb1008430f5263f772fe9869be1cdc1336a1c2414f7fdb9423"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('twichAI-ydQv36PI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
