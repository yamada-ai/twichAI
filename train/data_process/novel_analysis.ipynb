{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/share/virtualenvs/twitchAI-AEfI_JA6/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "和布\t名詞,一般,*,*,*,*,和布,ワカメ,ワカメ\n",
      "ちゃん\t名詞,接尾,人名,*,*,*,ちゃん,チャン,チャン\n",
      "こんにちは\t感動詞,*,*,*,*,*,こんにちは,コンニチハ,コンニチワ\n"
     ]
    }
   ],
   "source": [
    "from wakame.tokenizer import Tokenizer\n",
    "from wakame.analyzer import Analyzer\n",
    "from wakame.charfilter import *\n",
    "from wakame.tokenfilter import *\n",
    "\n",
    "text = '和布ちゃんこんにちは'\n",
    "\n",
    "# 基本的な使い方\n",
    "tokenizer = Tokenizer(use_neologd=True)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['和布', 'ちゃん', 'こんにちは']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"彼の技術は極めて優れている\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['彼', 'の', '技術', 'は', '極めて', '優れ', 'て', 'いる']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2morpheme(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_path = \"../../corpus/novel2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncodes= [x  for x in os.listdir(novel_path) if \".\" not in x ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_EOS(sentence):\n",
    "    if sentence[-1] in [\"。\", \"?\", \"!\", \"．\"]:\n",
    "        return sentence\n",
    "    else:\n",
    "         return sentence+\"。\"\n",
    "\n",
    "def is_valid_utterance_segment(text:str)->bool:\n",
    "    # 複数の\"「\"には囲まれていない\n",
    "    if text.count(\"「\") == 1 and text.count(\"」\") == 1 and text[0]==\"「\" and text[-1]==\"」\":\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convs = []\n",
    "    # for text in tqdm(novel):\n",
    "        # if is_valid_utterance_segment(text):\n",
    "        #     utt = re.search(pattern, text).group()\n",
    "        #     utt = clean_text_plain(utt)\n",
    "        #     utt = format_EOS( utt )\n",
    "        #     convs.append(utt)\n",
    "        # print(text)\n",
    "    # print(convs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_plain(text):\n",
    "    text_ = neologdn.normalize(text)\n",
    "    # text_ = re.sub(r'\\([^\\)]*\\)', \"\", text_)\n",
    "    # text_ = re.sub(r'\\([^\\)]*\\)', \"\", text_)\n",
    "    text_ = re.sub(r'\\d+', \"0\", text_)\n",
    "    if \"……\" in text_:\n",
    "        text_ = text_.replace(\"……\", \"…\")\n",
    "    return text_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1ページの小説をデータに変換\n",
    "import numpy as np\n",
    "import copy\n",
    "pattern = '(?<=\\「).*(?=\\」)'\n",
    "def novel2data(novel:list):\n",
    "    data_list = []\n",
    "    \"\"\"\n",
    "    1 : 会話\n",
    "    0 : それ以外\n",
    "\n",
    "    011, 101, 111 の時に登録可能とする\n",
    "    \n",
    "    \"\"\"\n",
    "    # 1, 0 の登録\n",
    "    # zero_one_vector = np.zeros(len(novel))\n",
    "    zero_one = \"\"\n",
    "    novel_normalized = []\n",
    "\n",
    "    conv_data = []\n",
    "\n",
    "    for i, line in enumerate(novel):\n",
    "        # 普通の発話なら1\n",
    "        if is_valid_utterance_segment(line):\n",
    "            zero_one += \"1\"\n",
    "            utt = re.search(pattern, line).group()\n",
    "            # そもそも存在しないのであれば\n",
    "            if utt==\"\":\n",
    "                # print(line)\n",
    "                continue\n",
    "            utt = clean_text_plain(utt)\n",
    "            if utt==\"\":\n",
    "                utt = re.search(pattern, line).group()\n",
    "            utt = format_EOS( utt )\n",
    "            novel_normalized.append(utt)\n",
    "        else:\n",
    "            zero_one += \"0\"\n",
    "            novel_normalized.append(clean_text_plain(line))\n",
    "\n",
    "    descriptive = []\n",
    "    # 部分を検索\n",
    "    for i in range(2, len(novel_normalized)):\n",
    "        if zero_one[i]==\"0\":\n",
    "            descriptive.append(novel_normalized[i])\n",
    "        comp = zero_one[i-3:i]\n",
    "        if \"011\" == comp:\n",
    "            conv_data.append(descriptive[-3:] + novel_normalized[i-2:i])\n",
    "            \n",
    "        elif \"101\" == comp:\n",
    "            conv_data.append(descriptive[-3:] +[novel_normalized[i-3], novel_normalized[i-1]])\n",
    "\n",
    "        elif \"111\" == comp:\n",
    "            conv_data.append(descriptive[-3:] + novel_normalized[i-2:i])\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return conv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 131/500 [00:11<00:34, 10.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「」\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 149/500 [00:13<00:34, 10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「」\n",
      "「」\n",
      "「」\n",
      "「」\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 192/500 [00:17<00:26, 11.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「」\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 363/500 [00:32<00:15,  8.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「」\n",
      "「」\n",
      "「」\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 434/500 [00:39<00:07,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「」\n",
      "「」\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:45<00:00, 11.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for ncode in tqdm(ncodes):\n",
    "    for number in sorted(os.listdir(novel_path+ncode) ,reverse=True):\n",
    "        with open(novel_path+ncode+\"/\"+number, \"r\") as f:\n",
    "            novel = f.read().splitlines()\n",
    "            conv_data += novel2data(novel) \n",
    "        # break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1045266"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_path = \"../../corpus/novel_formated/\"\n",
    "os.listdir(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_name = \"novel_segments.tsv\"\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(out_path+corpus_name, \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    writer.writerows(conv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b768359a65658dcda07a01f9fcd77d023c140b301dc2847ae2491657ec52602"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('twitchAI-AEfI_JA6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
